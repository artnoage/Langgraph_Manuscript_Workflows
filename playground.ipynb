{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from simple_workflows import *\n",
    "from simple_tools import *\n",
    "from workflows_as_tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is a multiagent workflow. Its purpose is to retrieve a collection of papers from arxiv.\n",
    "### The input is a 'list' (in the sense of everyday speech) with each element being the name or keywords around the paper (check the bib file).\n",
    "### Under the hood it searches for the most relevant paper and downlads it in the pdf folder.\n",
    "### In the end you get a report of the papers that were retrieved.\n",
    "### This needs an OpenAI API key to work. There are ways around it, but you need to use a Chat method that uses tools. \n",
    "### You can try your own bibliography here. example bib={1. life of brian, 2. death rebearth 3.  time  illustion wondering face }\n",
    "\n",
    "input={\"receptionist_retriever_history\":[HumanMessage(content=\"\")],\n",
    "    \"last_action_outcome\":[HumanMessage(content=\"\")],\n",
    "    \"metadata\":HumanMessage(content=\" \"),\n",
    "    \"article_keywords\":HumanMessage(content=\" \"),\n",
    "    \"title_of_retrieved_paper\":HumanMessage(content=\" \"),\n",
    "    \"should_I_clean\": False}\n",
    "input[\"receptionist_retriever_history\"][0]=HumanMessage(content=\"Please fetch me the following papers:\" + \"1 mitochondira are really small? , 2 is life really short?\")\n",
    "\n",
    "### Here You can set different agents to staff the workflow. The default is arxiv_retriever_workflow(retrieval_model=ChatOpenAI(model=\"gpt-3.5-turbo\",temperature=0), \n",
    "### cleaner_model=ChatNVIDIA(model=\"meta/llama3-70b-instruct\"), receptionist_model=ChatNVIDIA(model=\"meta/llama3-70b-instruct\"))\n",
    "### the retrieval agents needs tools and ChatNVIDIA is still in development. \n",
    "\n",
    "retrieve_app=ArxivRetrievalWorkflow()\n",
    "retrieve_app=retrieve_app.create_workflow()\n",
    "retrieve_app=retrieve_app.compile()\n",
    "state=retrieve_app.invoke(input,{\"recursion_limit\": 100})    \n",
    "print(state[\"receptionist_retriever_history\"][-1].content)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Same as before but in a form of a tool. Just for testing purposes.\n",
    "ArxivRetrievalTool=ArxivRetrievalToolClass()\n",
    "ArxivRetrievalTool=StructuredTool(name=\"ArxivRetrievalTool\",func=ArxivRetrievalTool.retrieve_bib,args_schema=ArxivRetrievalInput,\n",
    "                           description=ArxivRetrievalTool.description)\n",
    "ArxivRetrievalTool.invoke(\"1.Creatine for gains, 2. Is time relative or relativety had its time\")\n",
    "             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### It takes the name of a pdf located in the folder files\\pdfs as inpup and creates two markdowns, one with mupdf and one with nougat.\n",
    "pdf_to_markdown.invoke(\"Li\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This tool takes two version of the same file and creates a new one that actually has the best of both worlds\n",
    "### As it is, it just fixes an issue with citation format with nougat by leveraging the ocr one gets from mupdf\n",
    "###(which is lower quality but with the right format). With a little bit of prompting tweeking it can get more\n",
    "### general tasks of this nature. This tool needs an embeding mechanism as well. The reason is to locate the pages\n",
    "### in the second file that correspond to the pages in the first.\n",
    "\n",
    "ocr_enhancer_app=OcrEnchancingWorkflow()\n",
    "ocr_enhancer_app=ocr_enhancer_app.create_workflow()\n",
    "ocr_enhancer_app=ocr_enhancer_app.compile()\n",
    "input={\"main_text_filename\": HumanMessage(content=\"Li\"), \"supporting_text_filename\": HumanMessage(content=\"mu_Li\")}\n",
    "state=ocr_enhancer_app.invoke(input)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Same as before but in a form of a tool. Just for testing purposes.\n",
    "\n",
    "OcrEnhancingTool=OcrEnhancingToolClass()\n",
    "OcrEnhancingTool=StructuredTool(name=\"OcrEnhancingTool\",func=OcrEnhancingTool.ocr_enhance,args_schema=OcrEnhancingInput)\n",
    "OcrEnhancingTool.invoke({\"main_text_filename\": \"Li\", \"supporting_text_filename\": \"mu_Li\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The idea of this chain/tool is to remove the proofs from a paper so it will be easier to make a summary out of it. \n",
    "### The tool uses two chains under the hood. The first stamps the pages of the text that continue a proof from the previous page.\n",
    "### The idea was to help the LLM a bit to recognize proofs. The second LLM is doint the removal.\n",
    "### From all the modules I created, this was the most unsucessful one. Even with strong LLMs like GPT-4o and Opus, I had partial results.\n",
    "### I welcome anyone who can improve the prompt for this tool.\n",
    "proof_remover_app=ProofRemovingWorkflow()\n",
    "proof_remover_app=proof_remover_app.create_workflow()\n",
    "proof_remover_app=proof_remover_app.compile()\n",
    "input={\"main_text_filename\": HumanMessage(content=\"Li\"),\"file\":[\"\"],\"report\":HumanMessage(content=\"\")}\n",
    "state=proof_remover_app.invoke(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [02:44<00:00,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proofs were remove and the resulted file is named Li_without_proofs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The proofs were remove and the resulted file is named Li_without_proofs'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Same as before but in a form of a tool. Just for testing purposes.\n",
    "\n",
    "ProofRemoverTool=ProofRemovalToolClass()\n",
    "ProofRemoverTool=StructuredTool(name=\"ProofRemovalTool\",func=ProofRemoverTool.remove_proof,args_schema=ProofRemovalInput)\n",
    "ProofRemoverTool.invoke({\"main_text_filename\": \"Li\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This tool takes a text found in the folder files/markdowns and creates a set of keywords and a summary. in a form of a string and extracts the keywords and summary.\n",
    "### It is preferable to use a file that doesnt contain proofs because it produces a better summary. \n",
    "input={\"main_text_filename\": HumanMessage(content=\"Li\"),\n",
    "           \"report\":HumanMessage(content=\"\"),}\n",
    "keyword_and_summary_app=KeywordAndSummaryWorkflow()\n",
    "keyword_and_summary_app=keyword_and_summary_app.create_workflow()\n",
    "keyword_and_summary_app=keyword_and_summary_app.compile()\n",
    "state=keyword_and_summary_app.invoke(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyword_and_summary in progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 11/34 [01:33<03:50, 10.00s/it]"
     ]
    }
   ],
   "source": [
    "### Same as before but in a form of a tool. Just for testing purposes.\n",
    "\n",
    "KeywordAndSummaryTool=KeywordAndSummaryToolClass()\n",
    "KeywordAndSummaryTool=StructuredTool(name=\"KeywordAndSummaryTool\",func=KeywordAndSummaryTool.get_keyword_and_summary,args_schema=KeywordSummaryInput)\n",
    "KeywordAndSummaryTool.invoke({\"main_text_filename\": \"Li\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input={\"keywords_and_summary_filename\": HumanMessage(content=\"markdowns\\Li_keyword_and_summary\"), \"target_language\":HumanMessage\n",
    "(content=\"en\"),\"main_text_filename\": HumanMessage(content=\"Li\"), \"report\":HumanMessage}\n",
    "\n",
    "translation_app=TranslationWorkflow()\n",
    "translation_app=translation_app.create_workflow()\n",
    "translation_app=translation_app.compile()\n",
    "state=translation_app.invoke(input)\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: The keyword_and_summary file does not exist. Assuming keyword_and_summary is blank.\n",
      "Translation of Li in progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 19/34 [02:51<04:08, 16.54s/it]"
     ]
    }
   ],
   "source": [
    "TranslationTool =TranslationToolClass()\n",
    "    \n",
    "TranslationTool=StructuredTool(name=\"TranslationTool\",func=TranslationTool.translate_file,args_schema=TranslatorInput,\n",
    "                           description=TranslationTool.description)\n",
    "TranslationTool.invoke(input={\"keywords_and_summary_filename\":\"\",\"target_language\":\"en\",\"main_text_filename\":\"Li\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vaios",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
